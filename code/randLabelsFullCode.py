# -*- coding: utf-8 -*-
"""Random Final1_Letter_DL_Baseline_Comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zgu1VTgV8NIEaljIuZw26DPP9KzTL-9x
"""

!pip install transformers==4.30
!pip install torch
!pip install tensorflow
!pip install transformers[torch]
!pip install -U datasets


from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
import tensorflow as tf
import random
tokenizer = T5Tokenizer.from_pretrained('t5-small')
baseline = T5ForConditionalGeneration.from_pretrained('t5-small')
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

import random

def generate_random_labels(digits):
    """Generates a random set of unique labels for the given number of digits."""
    letters = random.sample([chr(i) for i in range(65, 91)], digits)
    return letters

def format_number_with_specified_markers(number, digits, labels):
    """Formats the number with leading zeros and attaches specified markers to each digit."""
    number_str = f"{number:0{digits}d}"
    return ' '.join(f"{label} {digit}" for label, digit in zip(labels, number_str))

def generate_addition_example_with_variable_digits(min_digits, max_digits):
    digits1 = random.randint(min_digits, max_digits)
    digits2 = random.randint(min_digits, max_digits)
    num1 = random.randint(0, 10**digits1 - 1)
    num2 = random.randint(0, 10**digits2 - 1)

    max_digits_num1_num2 = max(digits1, digits2)
    labels = generate_random_labels(max_digits_num1_num2)

    # Ensure both numbers are formatted to the same number of digits
    formatted_num1 = format_number_with_specified_markers(num1, max_digits_num1_num2, labels)
    formatted_num2 = format_number_with_specified_markers(num2, max_digits_num1_num2, labels)

    problem = f"{formatted_num1} + {formatted_num2}"

    solution = num1 + num2
    solution_digits = max_digits_num1_num2 + (solution >= 10**max_digits_num1_num2)

    if solution_digits > max_digits_num1_num2:
        # Add one additional random label if the solution length increases
        new_label = random.choice([chr(i) for i in range(65, 91) if chr(i) not in labels])
        labels.insert(0, new_label)  # Insert the new label at the beginning

    solution_formatted = format_number_with_specified_markers(solution, solution_digits, labels)

    return (problem, solution_formatted)

def generate_addition_examples_with_variable_digits(num_samples, min_digits, max_digits):
    return [generate_addition_example_with_variable_digits(min_digits, max_digits) for _ in range(num_samples)]

def save_dataset_to_file(dataset, filename):
    with open(filename, "w") as file:
        for problem, solution in dataset:
            file.write(f"{problem} = {solution}\n")

def generate_and_save_datasets(num_samples, filename):
    # Generate in-distribution dataset (1 to 5 digits)
    in_distribution_dataset = generate_addition_examples_with_variable_digits(num_samples // 2, 1, 5)
    save_dataset_to_file(in_distribution_dataset, f"{filename}_in_distribution.txt")

    # Generate out-of-distribution dataset (6 to 10 digits)
    out_of_distribution_dataset = generate_addition_examples_with_variable_digits(num_samples // 2, 6, 10)
    save_dataset_to_file(out_of_distribution_dataset, f"{filename}_out_of_distribution.txt")

# Parameters
num_samples = 10000  # Number of total samples
filename = "t5_dataset"

# Generate and save the datasets
generate_and_save_datasets(num_samples, filename)

def preprocess_function(examples):
    # Access data from the dictionary format in the Hugging Face dataset
    problems = examples['problem']
    solutions = examples['solution']

    # Format the problems into model inputs
    inputs = [f"add: {problem} =" for problem in problems]
    model_inputs = tokenizer(
        inputs,
        max_length=16,
        truncation=True,
        padding='max_length'
    )

    # Prepare labels for the model
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            solutions,
            max_length=16,
            truncation=True,
            padding='max_length'
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Load in-distribution dataset
def load_dataset_from_file(filename):
    dataset = []
    with open(filename, "r") as file:
        for line in file:
            problem, solution = line.strip().split(" = ")  # Correct the split pattern here
            dataset.append({"problem": problem, "solution": solution})  # Convert to dictionary for easier data handling
    return dataset

train_dataset = load_dataset_from_file("t5_dataset_in_distribution.txt")
print(train_dataset)

train_dataset, eval_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42)

from datasets import Dataset
train_dataset_hf = Dataset.from_dict({"problem": [x['problem'] for x in train_dataset], "solution": [x['solution'] for x in train_dataset]})
eval_dataset_hf = Dataset.from_dict({"problem": [x['problem'] for x in eval_dataset], "solution": [x['solution'] for x in eval_dataset]})
tokenized_train = train_dataset_hf.map(preprocess_function, batched=True, num_proc=16)
tokenized_eval = eval_dataset_hf.map(preprocess_function, batched=True, num_proc=16)

model = T5ForConditionalGeneration.from_pretrained("t5-small")
# Set up training arguments
training_args = TrainingArguments(
    output_dir='./results',          # Directory to store model checkpoints and results
    evaluation_strategy="epoch",     # Evaluate model at the end of each epoch
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,   # Adjust based on available GPU memory
    per_device_eval_batch_size=16,
    num_train_epochs=200,              # Number of training epochs
    weight_decay=0.01                # Weight decay for regularization
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval  # Use the tokenized evaluation dataset
)

trainer.train()
# Save the final model after training
trainer.save_model('./results/final_model')

# Load the model from the final checkpoint
model_path = './results/final_model'
trained_model = T5ForConditionalGeneration.from_pretrained(model_path)
trained_tokenizer = T5Tokenizer.from_pretrained("t5-small")

def add_numbers(expression, model, tokenizer):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Choose device according to availability
    model.to(device)  # Move model to the chosen device

    input_text = f"add: {expression} ="
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=16, truncation=True)
    input_ids = input_ids.to(device)  # Move input tensor to the same device

    # Generate output using the model
    outputs = model.generate(input_ids)
    predicted_result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return predicted_result
example_expression = "B 1 A 1 + B 2 A 9"
result = add_numbers(example_expression, model, trained_tokenizer)
print(f"{example_expression} = {result}")

example_expression = "B 6 + A 3 + B 4 A 6"
result = add_numbers(example_expression, model, trained_tokenizer)
print(f"{example_expression} = {result}")

# Load the model from the final checkpoint
model_path = './results/final_model'
trained_model = T5ForConditionalGeneration.from_pretrained(model_path)
trained_tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Example usage of the trained model
def add_numbers(expression, model, tokenizer):
    input_text = f"add: {expression} ="
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=16, truncation=True)

    outputs = model.generate(input_ids)
    predicted_result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return predicted_result

example_expression = "500+12"
result = add_numbers(example_expression, trained_model, trained_tokenizer)
print(f"{example_expression} = {result}")

from google.colab import drive
drive.mount('/content/drive')

from transformers import T5ForConditionalGeneration, T5Tokenizer

# Define the local path in the mounted Google Drive
path = '/content/drive/My Drive/DataC'  # Change 'DataF' to your specific folder

# Save model and tokenizer to the defined path
model.save_pretrained(path)
tokenizer.save_pretrained(path)



# import zipfile
# import os

# # Function to zip the directory
# def zipdir(path, ziph):
#     # ziph is zipfile handle
#     for root, dirs, files in os.walk(path):
#         for file in files:
#             # Create a proper path
#             ziph.write(os.path.join(root, file),
#                        os.path.relpath(os.path.join(root, file),
#                                        os.path.join(path, '..')))

# # File paths
# directory_path = '/content/drive/My Drive/DataF'
# zip_file_path = '/content/DataF.zip'

# # Creating zip file
# zipf = zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED)
# zipdir(directory_path, zipf)
# zipf.close()

# # Code to download the zip file to local system
# from google.colab import files
# files.download('/content/DataF.zip')

from google.colab import drive
drive.mount('/content/drive')

# Define the directory path where you want to save the model and tokenizer
save_directory = '/content/drive/My Drive/your_folder_name'

# Ensure the directory exists
import os
if not os.path.exists(save_directory):
    os.makedirs(save_directory)

# Save the model and tokenizer
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# List all files in the save directory to verify
print(os.listdir(save_directory))

from google.colab import drive
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import random
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

# Mount Google Drive and set the directory
drive.mount('/content/drive')
model_path = '/content/drive/My Drive/your_folder_name'

# Load the model and tokenizer from the specified path
trained_model = T5ForConditionalGeneration.from_pretrained(model_path)
trained_tokenizer = T5Tokenizer.from_pretrained(model_path)

import random
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
from tqdm import tqdm
import matplotlib.pyplot as plt

# Define the maximum number of digits to be evaluated
num_digits_max = 25

def generate_random_labels(digits):
    """Generates a random set of unique labels for the given number of digits."""
    letters = random.sample([chr(i) for i in range(65, 91)], digits)
    return letters

def format_number_with_specified_markers(number, digits, labels):
    """Formats the number with leading zeros and attaches specified markers to each digit."""
    number_str = f"{number:0{digits}d}"
    return ' '.join(f"{label} {digit}" for label, digit in zip(labels, number_str))

def generate_addition_example_with_variable_digits(min_digits, max_digits):
    digits1 = random.randint(min_digits, max_digits)
    digits2 = random.randint(min_digits, max_digits)
    num1 = random.randint(0, 10**digits1 - 1)
    num2 = random.randint(0, 10**digits2 - 1)

    max_digits_num1_num2 = max(digits1, digits2)
    labels = generate_random_labels(max_digits_num1_num2)

    # Ensure both numbers are formatted to the same number of digits
    formatted_num1 = format_number_with_specified_markers(num1, max_digits_num1_num2, labels)
    formatted_num2 = format_number_with_specified_markers(num2, max_digits_num1_num2, labels)

    problem = f"{formatted_num1} + {formatted_num2}"

    solution = num1 + num2
    solution_digits = max_digits_num1_num2 + (solution >= 10**max_digits_num1_num2)

    if solution_digits > max_digits_num1_num2:
        # Add one additional random label if the solution length increases
        possible_labels = [chr(i) for i in range(65, 91) if chr(i) not in labels]
        if possible_labels:
            new_label = random.choice(possible_labels)
            labels.insert(0, new_label)  # Insert the new label at the beginning

    solution_formatted = format_number_with_specified_markers(solution, solution_digits, labels)

    return (problem, solution_formatted)

# Generate formatted addition examples for testing with expected results also formatted
def generate_formatted_addition_example(num_digits):
    num1 = random.randint(10**(num_digits-1), 10**num_digits - 1)
    num2 = random.randint(10**(num_digits-1), 10**num_digits - 1)

    labels = generate_random_labels(num_digits)

    formatted_problem = f"{format_number_with_specified_markers(num1, num_digits, labels)} + {format_number_with_specified_markers(num2, num_digits, labels)}"

    solution = num1 + num2
    solution_digits = len(str(solution))

    if solution_digits > num_digits:
        # Add one additional random label if the solution length increases
        possible_labels = [chr(i) for i in range(65, 91) if chr(i) not in labels]
        if possible_labels:
            new_label = random.choice(possible_labels)
            labels.insert(0, new_label)  # Insert the new label at the beginning

    formatted_solution = format_number_with_specified_markers(solution, solution_digits, labels)

    return (formatted_problem, formatted_solution)

def extract_digits(formatted_string):
    """Extracts and returns only the digits from a formatted string."""
    return ''.join(filter(str.isdigit, formatted_string))

# Evaluate model performance on formatted data for multiple digit numbers
def evaluate_formatted_model_performance(trained_model, trained_tokenizer, num_classes=num_digits_max, examples_per_class=100):
    accuracy_per_class = []
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    trained_model.to(device)
    print("Testing individual examples...")

    for num_digits in range(1, num_classes + 1):
        correct_predictions = 0
        examples = [generate_formatted_addition_example(num_digits) for _ in range(examples_per_class)]

        for problem, expected_result in tqdm(examples, desc=f"Evaluating {num_digits}-digit examples"):
            input_text = f"add: {problem} ="
            input_ids = trained_tokenizer.encode(input_text, return_tensors="pt", max_length=64, truncation=True).to(device)
            outputs = trained_model.generate(input_ids)
            predicted_result = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Extract digits for comparison
            expected_digits = extract_digits(expected_result)
            predicted_digits = extract_digits(predicted_result)

            if predicted_digits == expected_digits:
                correct_predictions += 1

        accuracy = correct_predictions / examples_per_class
        accuracy_per_class.append(accuracy)

    return accuracy_per_class

# Load your model and tokenizer from a specific path
model_path = '/content/drive/My Drive/your_folder_name'
trained_model = T5ForConditionalGeneration.from_pretrained(model_path)
trained_tokenizer = T5Tokenizer.from_pretrained(model_path)

# Perform the evaluation for all classes
accuracy_per_class = evaluate_formatted_model_performance(trained_model, trained_tokenizer)

# Plotting the accuracy vs. number of digits
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_digits_max + 1), accuracy_per_class, marker='o')
plt.xlabel('Number of Digits')
plt.ylabel('Accuracy')
plt.title('Model Accuracy vs. Number of Digits')
plt.xticks(range(1, num_digits_max + 1))
plt.grid(True)
plt.show()