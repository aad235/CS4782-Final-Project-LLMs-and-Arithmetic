# -*- coding: utf-8 -*-
"""Final2_Letter_DL_Baseline_Comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qruQLe34feWrFit8d3qr3rNArAbn0Y_3
"""

!pip install transformers==4.30
!pip install torch
!pip install tensorflow
!pip install transformers[torch]
!pip install -U datasets


from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
import tensorflow as tf
import random
tokenizer = T5Tokenizer.from_pretrained('t5-small')
baseline = T5ForConditionalGeneration.from_pretrained('t5-small')
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split

import random

def format_number_with_markers(number, digits):
    """Formats the number with leading zeros and attaches markers to each digit according to its position."""
    number_str = f"{number:0{digits}d}"
    # Each digit gets a marker, starting with the most significant digit
    return ' '.join(f"{chr(64 + digits - i)} {digit}" for i, digit in enumerate(number_str))

def generate_addition_example_with_variable_digits(min_digits, max_digits):
    digits1 = random.randint(min_digits, max_digits)
    digits2 = random.randint(min_digits, max_digits)
    num1 = random.randint(0, 10**digits1 - 1)
    num2 = random.randint(0, 10**digits2 - 1)
    problem = f"{format_number_with_markers(num1, digits1)} + {format_number_with_markers(num2, digits2)}"
    solution = num1 + num2
    solution_digits = max(digits1, digits2) + (solution >= 10**max(digits1, digits2))
    solution_formatted = format_number_with_markers(solution, solution_digits)
    return (problem, solution_formatted)

def generate_addition_examples_with_variable_digits(num_samples, min_digits, max_digits):
    return [generate_addition_example_with_variable_digits(min_digits, max_digits) for _ in range(num_samples)]

def save_dataset_to_file(dataset, filename):
    with open(filename, "w") as file:
        for problem, solution in dataset:
            file.write(f"{problem} = {solution}\n")

def generate_and_save_datasets(num_samples, filename):
    # Generate in-distribution dataset (1 to 5 digits)
    in_distribution_dataset = generate_addition_examples_with_variable_digits(num_samples // 2, 1, 5)
    save_dataset_to_file(in_distribution_dataset, f"{filename}_in_distribution.txt")

    # Generate out-of-distribution dataset (6 to 10 digits)
    out_of_distribution_dataset = generate_addition_examples_with_variable_digits(num_samples // 2, 6, 10)
    save_dataset_to_file(out_of_distribution_dataset, f"{filename}_out_of_distribution.txt")

# Parameters
num_samples = 10000  # Number of total samples
filename = "t5_dataset"

# Generate and save the datasets
generate_and_save_datasets(num_samples, filename)

def preprocess_function(examples):
    # Access data from the dictionary format in the Hugging Face dataset
    problems = examples['problem']
    solutions = examples['solution']

    # Format the problems into model inputs
    inputs = [f"add: {problem} =" for problem in problems]
    model_inputs = tokenizer(
        inputs,
        max_length=16,
        truncation=True,
        padding='max_length'
    )

    # Prepare labels for the model
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            solutions,
            max_length=16,
            truncation=True,
            padding='max_length'
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Load in-distribution dataset
def load_dataset_from_file(filename):
    dataset = []
    with open(filename, "r") as file:
        for line in file:
            problem, solution = line.strip().split(" = ")  # Correct the split pattern here
            dataset.append({"problem": problem, "solution": solution})  # Convert to dictionary for easier data handling
    return dataset

train_dataset = load_dataset_from_file("t5_dataset_in_distribution.txt")
print(train_dataset)

train_dataset, eval_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42)

from datasets import Dataset
train_dataset_hf = Dataset.from_dict({"problem": [x['problem'] for x in train_dataset], "solution": [x['solution'] for x in train_dataset]})
eval_dataset_hf = Dataset.from_dict({"problem": [x['problem'] for x in eval_dataset], "solution": [x['solution'] for x in eval_dataset]})
tokenized_train = train_dataset_hf.map(preprocess_function, batched=True, num_proc=16)
tokenized_eval = eval_dataset_hf.map(preprocess_function, batched=True, num_proc=16)

model = T5ForConditionalGeneration.from_pretrained("t5-small")
# Set up training arguments
training_args = TrainingArguments(
    output_dir='./results',          # Directory to store model checkpoints and results
    evaluation_strategy="epoch",     # Evaluate model at the end of each epoch
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,   # Adjust based on available GPU memory
    per_device_eval_batch_size=16,
    num_train_epochs=200,              # Number of training epochs
    weight_decay=0.01                # Weight decay for regularization
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval  # Use the tokenized evaluation dataset
)

trainer.train()
# Save the final model after training
trainer.save_model('./results/final_model')

# Load the model from the final checkpoint
model_path = './results/final_model'
trained_model = T5ForConditionalGeneration.from_pretrained(model_path)
trained_tokenizer = T5Tokenizer.from_pretrained("t5-small")

def add_numbers(expression, model, tokenizer):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Choose device according to availability
    model.to(device)  # Move model to the chosen device

    input_text = f"add: {expression} ="
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=16, truncation=True)
    input_ids = input_ids.to(device)  # Move input tensor to the same device

    # Generate output using the model
    outputs = model.generate(input_ids)
    predicted_result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return predicted_result
example_expression = "B 1 A 1 + B 2 A 9"
result = add_numbers(example_expression, model, trained_tokenizer)
print(f"{example_expression} = {result}")

example_expression = "B 6 + A 3 + B 4 A 6"
result = add_numbers(example_expression, model, trained_tokenizer)
print(f"{example_expression} = {result}")

# Load the model from the final checkpoint
model_path = './results/final_model'
trained_model = T5ForConditionalGeneration.from_pretrained(model_path)
trained_tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Example usage of the trained model
def add_numbers(expression, model, tokenizer):
    input_text = f"add: {expression} ="
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=16, truncation=True)

    outputs = model.generate(input_ids)
    predicted_result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return predicted_result

example_expression = "500+12"
result = add_numbers(example_expression, trained_model, trained_tokenizer)
print(f"{example_expression} = {result}")

from google.colab import drive
drive.mount('/content/drive')

from transformers import T5ForConditionalGeneration, T5Tokenizer

# Define the local path in the mounted Google Drive
path = '/content/drive/My Drive/DataC'  # Change 'DataF' to your specific folder

# Save model and tokenizer to the defined path
model.save_pretrained(path)
tokenizer.save_pretrained(path)



# import zipfile
# import os

# # Function to zip the directory
# def zipdir(path, ziph):
#     # ziph is zipfile handle
#     for root, dirs, files in os.walk(path):
#         for file in files:
#             # Create a proper path
#             ziph.write(os.path.join(root, file),
#                        os.path.relpath(os.path.join(root, file),
#                                        os.path.join(path, '..')))

# # File paths
# directory_path = '/content/drive/My Drive/DataF'
# zip_file_path = '/content/DataF.zip'

# # Creating zip file
# zipf = zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED)
# zipdir(directory_path, zipf)
# zipf.close()

# # Code to download the zip file to local system
# from google.colab import files
# files.download('/content/DataF.zip')

from google.colab import drive
drive.mount('/content/drive')

# Define the directory path where you want to save the model and tokenizer
save_directory = '/content/drive/My Drive/your_folder_name'

# Ensure the directory exists
import os
if not os.path.exists(save_directory):
    os.makedirs(save_directory)

# Save the model and tokenizer
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# List all files in the save directory to verify
print(os.listdir(save_directory))

from google.colab import drive
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import random
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

# Mount Google Drive and set the directory
drive.mount('/content/drive')
model_path = '/content/drive/My Drive/your_folder_name'

# Load the model and tokenizer from the specified path
trained_model = T5ForConditionalGeneration.from_pretrained(model_path)
trained_tokenizer = T5Tokenizer.from_pretrained(model_path)

# Function to format numbers with markers
def format_number_with_markers(number, digits):
    """Formats the number with leading zeros and attaches markers to each digit according to its position."""
    number_str = f"{number:0{digits}d}"
    return ' '.join(f"{chr(64 + digits - i)} {digit}" for i, digit in enumerate(number_str))

# Generate formatted addition examples for testing with expected results also formatted
def generate_formatted_addition_example(num_digits):
    num1 = random.randint(10**(num_digits-1), 10**num_digits - 1)
    num2 = random.randint(10**(num_digits-1), 10**num_digits - 1)
    formatted_problem = f"{format_number_with_markers(num1, num_digits)} + {format_number_with_markers(num2, num_digits)}"
    solution = num1 + num2
    solution_digits = len(str(solution))
    formatted_solution = format_number_with_markers(solution, solution_digits)
    return (formatted_problem, formatted_solution)

# Evaluate model performance on formatted data for multiple digit numbers
def evaluate_formatted_model_performance(trained_model, trained_tokenizer, num_classes=30, examples_per_class=100):
    accuracy_per_class = []
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    trained_model.to(device)
    print("Testing individual examples...")

    for num_digits in range(1, num_classes + 1):
        correct_predictions = 0
        examples = [generate_formatted_addition_example(num_digits) for _ in range(examples_per_class)]

        for problem, expected_result in tqdm(examples, desc=f"Evaluating {num_digits}-digit examples"):
            input_text = f"add: {problem} ="
            input_ids = trained_tokenizer.encode(input_text, return_tensors="pt", max_length=64, truncation=True).to(device)
            outputs = trained_model.generate(input_ids)
            predicted_result = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)

            print(f"Problem: {problem} | Expected: {expected_result} | Predicted: {predicted_result}")

            if predicted_result == expected_result:
                correct_predictions += 1

        accuracy = correct_predictions / examples_per_class
        accuracy_per_class.append(accuracy)

    return accuracy_per_class

# Load your model and tokenizer from a specific path
model_path = '/content/drive/My Drive/your_folder_name'
trained_model = T5ForConditionalGeneration.from_pretrained(model_path)
trained_tokenizer = T5Tokenizer.from_pretrained(model_path)

# Perform the evaluation for all classes
accuracy_per_class = evaluate_formatted_model_performance(trained_model, trained_tokenizer)

# Plotting the accuracy vs. number of digits
plt.figure(figsize=(10, 6))
plt.plot(range(1, 31), accuracy_per_class, marker='o')
plt.xlabel('Number of Digits')
plt.ylabel('Accuracy')
plt.title('Model Accuracy vs. Number of Digits')
plt.xticks(range(1, 31))
plt.grid(True)
plt.show()